{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1043a0ca-d1ac-46b5-bb60-ee80747531b2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 提取MTA的feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b20b3-0375-4db8-a1a0-3bb8d0c97102",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 导入相应的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10f3e54-4fd4-432e-99ca-18c874f6a370",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('..')\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from Preprocess.util_ccc import concordance_correlation_coefficient\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc335e-8865-45e0-9f19-474a5c022056",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### 模型进行实例化，并将最优权重进行读取"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de0b7ff5-940c-4050-ac64-466ec60604b7",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "原始模型，没有输出最后的feature，需要对他进行保存。\n",
    "[] 后续可以采用更加简单的方式 进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7669cbe1-02ef-4741-9740-ac9cdc3b12aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# parameters\n",
    "## input_channels = [256,512,1024,2048]\n",
    "## attention_channels = 2048\n",
    "\n",
    "class NonLocalBlock(nn.Module):\n",
    "    \"\"\" NonLocalBlock Module\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(NonLocalBlock, self).__init__()\n",
    "\n",
    "        conv_nd = nn.Conv1d\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = self.in_channels // 2\n",
    "\n",
    "        self.ImageAfterASPP_bnRelu = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.DepthAfterASPP_bnRelu = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.R_g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "        self.R_theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                               kernel_size=1, stride=1, padding=0)\n",
    "        self.R_phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "        self.R_W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.F_g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "        self.F_theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                               kernel_size=1, stride=1, padding=0)\n",
    "        self.F_phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "        self.F_W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, self_fea, mutual_fea, alpha, selfImage):\n",
    "\n",
    "        if selfImage:\n",
    "\n",
    "            selfNonLocal_fea = self.ImageAfterASPP_bnRelu(self_fea)\n",
    "            mutualNonLocal_fea = self.DepthAfterASPP_bnRelu(mutual_fea)\n",
    "\n",
    "            batch_size = selfNonLocal_fea.size(0)\n",
    "            g_x = self.R_g(selfNonLocal_fea).view(batch_size, self.inter_channels, -1)\n",
    "            g_x = g_x.permute(0, 2, 1)\n",
    "            # using mutual feature to generate attention\n",
    "            theta_x = self.F_theta(mutualNonLocal_fea).view(batch_size, self.inter_channels, -1)\n",
    "            theta_x = theta_x.permute(0, 2, 1)\n",
    "            phi_x = self.F_phi(mutualNonLocal_fea).view(batch_size, self.inter_channels, -1)\n",
    "            f = torch.matmul(theta_x, phi_x)\n",
    "\n",
    "            # using self feature to generate attention\n",
    "            self_theta_x = self.R_theta(selfNonLocal_fea).view(batch_size, self.inter_channels, -1)\n",
    "            self_theta_x = self_theta_x.permute(0, 2, 1)\n",
    "            self_phi_x = self.R_phi(selfNonLocal_fea).view(batch_size, self.inter_channels, -1)\n",
    "            self_f = torch.matmul(self_theta_x, self_phi_x)\n",
    "            # add self_f and mutual f\n",
    "            f_div_C = F.softmax(alpha * f + self_f, dim=-1)\n",
    "            y = torch.matmul(f_div_C, g_x)\n",
    "            y = y.permute(0, 2, 1).contiguous()\n",
    "            y = y.view(batch_size, self.inter_channels, *selfNonLocal_fea.size()[2:])\n",
    "            W_y = self.R_W(y)\n",
    "            z = W_y + self_fea\n",
    "            return z\n",
    "\n",
    "        else:\n",
    "            selfNonLocal_fea = self.DepthAfterASPP_bnRelu(self_fea)## [30,2408,1]\n",
    "\n",
    "\n",
    "            mutualNonLocal_fea = self.ImageAfterASPP_bnRelu(mutual_fea)##[30,2048,1]\n",
    "\n",
    "            batch_size = selfNonLocal_fea.size(0) ##30\n",
    "\n",
    "            g_x = self.F_g(selfNonLocal_fea).view(batch_size, self.inter_channels, -1) ##[30,1,1024]\n",
    "            g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "            # using mutual feature to generate attention\n",
    "            theta_x = self.R_theta(mutualNonLocal_fea).view(batch_size, self.inter_channels, -1)\n",
    "            theta_x = theta_x.permute(0, 2, 1)\n",
    "            phi_x = self.R_phi(mutualNonLocal_fea).view(batch_size, self.inter_channels, -1)\n",
    "            f = torch.matmul(theta_x, phi_x)\n",
    "\n",
    "            # using self feature to generate attention\n",
    "            self_theta_x = self.F_theta(selfNonLocal_fea).view(batch_size, self.inter_channels, -1)\n",
    "            self_theta_x = self_theta_x.permute(0, 2, 1)\n",
    "            self_phi_x = self.F_phi(selfNonLocal_fea).view(batch_size, self.inter_channels, -1)\n",
    "            self_f = torch.matmul(self_theta_x, self_phi_x)\n",
    "\n",
    "            # add self_f and mutual f\n",
    "            f_div_C = F.softmax(alpha * f + self_f, dim=-1)\n",
    "            print(g_x.shape)\n",
    "            print(f_div_C.shape)\n",
    "            y = torch.matmul(f_div_C, g_x)\n",
    "            y = y.permute(0, 2, 1).contiguous()\n",
    "            y = y.view(batch_size, self.inter_channels, *selfNonLocal_fea.size()[2:])\n",
    "            W_y = self.F_W(y)\n",
    "            z = W_y + self_fea\n",
    "            return z\n",
    "\n",
    "class MTA(nn.Module):\n",
    "    def __init__(self,in_channel,input_channels,attention_channels,outchannels):\n",
    "        super(MTA, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        ## 定义多个channels，得到多尺度特征【Batch，256，1】，【Batch，512，1】，【Batch，1024，1】，【Batch，2048，1】\n",
    "        self.conv1 =  nn.ModuleList()\n",
    "        for i in input_channels:\n",
    "            temp_part = nn.Sequential(\n",
    "\n",
    "                nn.Conv1d(in_channels=in_channel, out_channels=i, kernel_size=1),\n",
    "                nn.BatchNorm1d(i),\n",
    "                nn.ReLU(inplace=True)\n",
    "\n",
    "            )\n",
    "            self.conv1.append(temp_part)\n",
    "\n",
    "        self.conv2 = nn.ModuleList()\n",
    "        for i in input_channels:\n",
    "            temp_part_2 = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=i, out_channels=attention_channels, kernel_size=1),\n",
    "                nn.BatchNorm1d(attention_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "\n",
    "            )\n",
    "            self.conv2.append(temp_part_2)\n",
    "        ## 通过attetnion 需要将他们对其到同一个尺度 return list[Batch,2048,1] * 4\n",
    "        self.conv3 = nn.Conv1d(in_channels= attention_channels *2,out_channels=2, kernel_size=1)\n",
    "        self.nonblock = NonLocalBlock(in_channels= attention_channels)\n",
    "        self.conv4 = nn.ModuleList()\n",
    "        for i in input_channels:\n",
    "            temp_part_4 = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=attention_channels, out_channels=outchannels, kernel_size=1),\n",
    "                nn.BatchNorm1d(outchannels),\n",
    "                nn.ReLU(inplace=True)\n",
    "\n",
    "            )\n",
    "            self.conv4.append(temp_part_4)\n",
    "\n",
    "        self.reg = nn.Sequential(\n",
    "            nn.Linear(in_features=outchannels * len(self.input_channels) * 30, out_features= 2048 ),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048,1)\n",
    "        )\n",
    "\n",
    "        # self.conv4 = nn.ModuleList([nn.Conv1d(in_channels= attention_channels,out_channels=outchannels,kernel_size=1) for i in range(len(input_channels))])\n",
    "\n",
    "    def  forward(self,x):\n",
    "        outs = [in_channel(x) for in_channel in self.conv1]\n",
    "        outs = [in_channel(outs[idx])for idx,in_channel in enumerate(self.conv2)]\n",
    "\n",
    "        if len(self.input_channels) == 4:\n",
    "            conncat_tensor_01 = torch.cat([outs[0], outs[1]], dim=1)\n",
    "            conncat_tensor_01_conv = self.conv3(conncat_tensor_01)\n",
    "            alpha_01 = F.softmax(conncat_tensor_01_conv,dim=1)\n",
    "            alpha_01_1 = alpha_01[:,1,:].unsqueeze(dim=2)\n",
    "            feature_attention_0 = self.nonblock(outs[0], outs[1], alpha_01_1, True)\n",
    "\n",
    "            conncat_tensor_12 = torch.cat([outs[1], outs[2]], dim=1)\n",
    "            conncat_tensor_12_conv = self.conv3(conncat_tensor_12)\n",
    "            alpha_12 = F.softmax(conncat_tensor_12_conv, dim=1)\n",
    "            alpha_12_2 = alpha_12[:, 1, :].unsqueeze(dim=2)\n",
    "            feature_attention_1 = self.nonblock(outs[1], outs[2], alpha_12_2, True)\n",
    "\n",
    "            conncat_tensor_23 = torch.cat([outs[2], outs[3]], dim=1)\n",
    "            conncat_tensor_23_conv = self.conv3(conncat_tensor_23)\n",
    "            alpha_23 = F.softmax(conncat_tensor_23_conv, dim=1)\n",
    "            alpha_23_3 = alpha_23[:, 1, :].unsqueeze(dim=2)\n",
    "            feature_attention_2 = self.nonblock(outs[2], outs[3], alpha_23_3, True)\n",
    "\n",
    "            conncat_tensor_30 = torch.cat([outs[3], outs[0]], dim=1)\n",
    "            conncat_tensor_30_conv = self.conv3(conncat_tensor_30)\n",
    "            alpha_30 = F.softmax(conncat_tensor_30_conv, dim=1)\n",
    "            alpha_30_3 = alpha_30[:, 1, :].unsqueeze(dim=2)\n",
    "            feature_attention_3 = self.nonblock(outs[3], outs[0], alpha_30_3, True)\n",
    "\n",
    "            outs = [feature_attention_0, feature_attention_1, feature_attention_2, feature_attention_3]\n",
    "        elif len(self.input_channels) == 3:\n",
    "            conncat_tensor_01 = torch.cat([outs[0], outs[1]], dim=1)\n",
    "            conncat_tensor_01_conv = self.conv3(conncat_tensor_01)\n",
    "            alpha_01 = F.softmax(conncat_tensor_01_conv, dim=1)\n",
    "            alpha_01_1 = alpha_01[:, 1, :].unsqueeze(dim=2)\n",
    "            feature_attention_0 = self.nonblock(outs[0], outs[1], alpha_01_1, True)\n",
    "\n",
    "            conncat_tensor_12 = torch.cat([outs[1], outs[2]], dim=1)\n",
    "            conncat_tensor_12_conv = self.conv3(conncat_tensor_12)\n",
    "            alpha_12 = F.softmax(conncat_tensor_12_conv, dim=1)\n",
    "            alpha_12_2 = alpha_12[:, 1, :].unsqueeze(dim=2)\n",
    "            feature_attention_1 = self.nonblock(outs[1], outs[2], alpha_12_2, True)\n",
    "\n",
    "            conncat_tensor_20 = torch.cat([outs[2], outs[0]], dim=1)\n",
    "            conncat_tensor_20_conv = self.conv2(conncat_tensor_20)\n",
    "            alpha_20 = F.softmax(conncat_tensor_20_conv, dim=1)\n",
    "            alpha_20_2 = alpha_20[:, 1, :].unsqueeze(dim=2)\n",
    "            feature_attention_2 = self.nonblock(outs[2], outs[0], alpha_20_2, True)\n",
    "\n",
    "            outs = [feature_attention_0, feature_attention_1, feature_attention_2]\n",
    "\n",
    "        outs = [in_channel(outs[idx]) for idx, in_channel in enumerate(self.conv4)]\n",
    "\n",
    "        input_feature = torch.cat(outs,dim = 1 )\n",
    "        input_feature = input_feature.view(input_feature.shape[0],-1)\n",
    "        outs = self.reg(input_feature)\n",
    "\n",
    "\n",
    "        return outs,input_feature,self.reg\n",
    "\n",
    "\n",
    "####　测试部分\n",
    "## 现在我们得到了多个尺度的特征\n",
    "#\n",
    "# x1 = torch.rand([30,2048,1])\n",
    "# x2 = torch.rand([30,2048,1])\n",
    "# x3 = torch.rand([30,2048,1])\n",
    "# x4 = torch.rand([30,2048,1])\n",
    "# outs = [x1,x2,x3,x4]\n",
    "# ### 1. concat feature\n",
    "#\n",
    "# conncat_tensor_01 = torch.cat([outs[0], outs[1]], dim=1)\n",
    "# print(conncat_tensor_01.shape)\n",
    "# conv1 = nn.Conv1d(in_channels=2048 *2 , out_channels=2, kernel_size=1)\n",
    "# conncat_tensor_01_conv = conv1(conncat_tensor_01)\n",
    "# alpha_01 = F.softmax(conncat_tensor_01_conv,dim=1)\n",
    "# alpha_0 = alpha_01[:,0,:]\n",
    "# alpha_1 = alpha_01[:,1,:].unsqueeze(dim=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# nonblock = NonLocalBlock(in_channels= 2048)\n",
    "# temp_feature_0 = nonblock(outs[0], outs[1], alpha_1, False)\n",
    "\n",
    "\n",
    "\n",
    "# conv1 = nn.Conv1d(in_channels=2048 * 2 ,out_channels=2,kernel_size=1)\n",
    "# print(conv1(conncat_tensor_01).shape)\n",
    "\n",
    "# feature = torch.rand([2,2048,30])\n",
    "# in_channel = 2048\n",
    "# input_channels = [256,512,1024,2048]\n",
    "# attention_channels = 2048\n",
    "# outchannels = 1024\n",
    "# model = MTA(in_channel = in_channel, input_channels=input_channels,attention_channels= attention_channels,outchannels=outchannels)\n",
    "# print(model)\n",
    "#\n",
    "# results = model(feature)\n",
    "#\n",
    "# print(results.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "447b3181-8d83-438c-934a-db37c69d944e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "in_channel = 2048\n",
    "input_channels = [256, 512, 1024, 2048]\n",
    "attention_channels = 2048\n",
    "outchannels = 512\n",
    "model = MTA(in_channel=in_channel, input_channels=input_channels, attention_channels=attention_channels,\n",
    "            outchannels=outchannels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a4a6440-face-4f6e-bc04-e29ae0fa8899",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/hy-tmp/Code/Best_weights/Resnet-MTA/Epcoh_159_Rmse_6.803299427032471_PCC_0.2584606433517226_CCC_0.15121600196380586.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bfe1bf-dd3d-4b91-bef6-78531340d3a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 数据集进行载入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570142e4-9e62-4e7e-84e0-7f04187493ec",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- [x] 这个时候发现一个现象，原始的dataset传入到的只有 feature & label。保存的时候 对名字也进行一个提取和保存"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8dc6774-4712-447b-b9ec-fd6704319e9c",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "！！！ 后续在写代码的时候，一定要在当前类，进行一个测试，方便后续使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85cd4e1b-9616-4042-bfad-1b8dde19f55e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import scipy.io as scio\n",
    "\n",
    "class DepressDataset(Dataset):\n",
    "    def __init__(self,txt_path):\n",
    "        self.txt_path = txt_path\n",
    "\n",
    "        with open(self.txt_path, 'r', encoding='utf-8') as f:\n",
    "            data = f.readlines()\n",
    "            features = [i.rstrip('\\n') for i in data]\n",
    "        self.features = features\n",
    "    def __getitem__(self, index):\n",
    "        data_file  = self.features[index]\n",
    "        \n",
    "        ## 提取sample_id\n",
    "        \n",
    "        sample_id = data_file.split('/')[-2]\n",
    "        \n",
    "        feature_name = data_file.split('/')[-1]\n",
    "        \n",
    "        data = scio.loadmat(data_file)  ## return 一个字典。选择‘feature’\n",
    "        feature = data['feature']  ##array 形式\n",
    "        feature_th = torch.from_numpy(feature)\n",
    "\n",
    "\n",
    "        label = data['label']\n",
    "        label_th = torch.from_numpy(label).reshape(-1).float()\n",
    "\n",
    "        return feature_th,label_th,sample_id,feature_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "502e585c-cd24-47ce-b6fd-2fa81f9a963a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1.mat', '2.mat', '3.mat', '4.mat', '5.mat', '6.mat', '7.mat', '8.mat', '9.mat', '10.mat')\n"
     ]
    }
   ],
   "source": [
    "#测试部分\n",
    "\n",
    "train_dataset = DepressDataset(txt_path= '../../train_data.txt')\n",
    "train_dataloader = DataLoader(dataset=train_dataset,batch_size=10,shuffle=False)\n",
    "for data in train_dataloader:\n",
    "    feature, label, sample_id,feature_name = data\n",
    "    print(feature_name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c00f51-016f-471c-9142-af14cecc4d27",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 开始模型的提取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9eff0-66a9-42bc-afa7-86956f652c26",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "由于每个样本数据，对应的特征数量是不一致的。因此，先对每个样本先单独保存，之后在进行聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26fc27f2-bd1f-4d56-9e3d-c2439c27e9ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_file = '../../train_valid.txt'\n",
    "\n",
    "train_batch_size = 1\n",
    "\n",
    "#### train dataloader\n",
    "train_dataset = DepressDataset(txt_path=train_file)\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39272f81-4036-48d8-9d0d-b6816ef1b3ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14ba8708-2d25-4a6b-8232-23926d86d4bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213322/213322 [42:35<00:00, 83.46it/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = '/hy-tmp/Feature_save_Resnet/'\n",
    "save_path_check = os.path.exists(save_path)\n",
    "\n",
    "if not save_path_check:\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for data in tqdm(train_loader):\n",
    "    \n",
    "    feature, label, sample_id,feature_name = data\n",
    "    \n",
    "    feature = feature.permute(0, 2, 1)\n",
    "    \n",
    "    feature = feature.to(device)\n",
    "    \n",
    "    _,mta_feature,reg_model = model(feature)\n",
    "    \n",
    "    reg_model.eval()\n",
    "    temp_model = reg_model[:-1]\n",
    "    \n",
    "    temp_feature = temp_model.forward(mta_feature).to('cpu').data.numpy()\n",
    "        \n",
    "    label = label.data.numpy()\n",
    "    \n",
    "    \n",
    " \n",
    "    save_dir = save_path + sample_id[0]\n",
    "    \n",
    "    save_dir_check = os.path.exists(save_dir)\n",
    "\n",
    "    if not save_dir_check:\n",
    "            os.mkdir(save_dir)\n",
    "\n",
    "    scio.savemat(pjoin(save_dir,feature_name[0]),{'feature':temp_feature, 'label': label})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e716fb5-1d34-4051-9b0d-75d32b2e4d04",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2048)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_file  = '/hy-tmp/Feature_save_Resnet/302/1.mat'\n",
    "\n",
    "mat_data = scio.loadmat(mat_file)\n",
    "\n",
    "mat_data['feature'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915ee82-dfd5-4bd5-8ee1-799b9f2b9f03",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 将每个文件夹的特征进行融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc16d0-3338-4b8c-a0f4-aa7b81c6a8cf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### numpy 对于空数组拼接。有点麻烦！！！ 采用tensor 进行拼接 转化为numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a21da73-326c-4bf9-bf2a-b327f2227dc5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [01:35<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "### 1. 读取文件夹内容。\n",
    "\n",
    "mat_path = '/hy-tmp/Feature_save_Resnet/'\n",
    "\n",
    "mat_file = [i for i in os.listdir(mat_path)]\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 定义 fusion feature path\n",
    "\n",
    "mta_fusion_dir = '/hy-tmp/Feature_fusion_resnet_mta_save/'\n",
    "\n",
    "mta_fusion_dir_check = os.path.exists(mta_fusion_dir)\n",
    "\n",
    "if not mta_fusion_dir_check:\n",
    "    os.mkdir(mta_fusion_dir)\n",
    "\n",
    "\n",
    "### 2. 读取对应的feature & label\n",
    "\n",
    "for file in tqdm(mat_file):\n",
    "    \n",
    "    mat_full_path = pjoin(mat_path,file)\n",
    "    \n",
    "    mat_data = [ pjoin(mat_full_path,temp_file ) for temp_file in os.listdir(mat_full_path)]\n",
    "    \n",
    "    ### data_file:/hy-tmp/Feature_save/302/1.mat\n",
    "\n",
    "    total_feature = torch.Tensor()\n",
    "    \n",
    "    total_label = torch.Tensor()\n",
    "    \n",
    "    \n",
    "    for data_file in mat_data:\n",
    "        \n",
    "        data = scio.loadmat(data_file)\n",
    "        \n",
    "        mat_feature = data['feature']\n",
    "        \n",
    "        mat_label = data['label']\n",
    "        \n",
    "        \n",
    "        total_feature = torch.cat([total_feature,torch.from_numpy(mat_feature)], dim= 0)\n",
    "        \n",
    "        total_label= torch.cat([total_label,torch.from_numpy(mat_label)], dim= 0)\n",
    "    \n",
    "    save_name = file + '.mat'\n",
    "\n",
    "    scio.savemat(pjoin(mta_fusion_dir,save_name),{'feature':total_feature.numpy(), 'label':mat_label, 'check_label':total_label.numpy()})\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9060d-4347-4990-9459-498faf62832a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "####  检验特征的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63a66d45-a5d6-4f7d-8c6c-0eee543c171b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Mon Aug 29 18:06:43 2022',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'feature': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'label': array([[2.]], dtype=float32),\n",
       " 'check_label': array([[2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.]], dtype=float32)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_mat_file = '/hy-tmp/Feature_fusion_resnet_mta_save/300.mat'\n",
    "\n",
    "check_mat = scio.loadmat(check_mat_file)\n",
    "\n",
    "check_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3bea99f4-baff-4163-8675-c71a19b7ee95",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(648, 1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_mat['check_label'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc9662-b09c-4caa-bb53-b675d6dd9b2b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 后续使用方便，将训练&验证，测试的样本id，进行保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b0e59d3-b167-4c5b-9172-ca658de22774",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>301</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>302</td>\n",
       "      <td>758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>303</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>304</td>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>713</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>271</td>\n",
       "      <td>715</td>\n",
       "      <td>1303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>272</td>\n",
       "      <td>716</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>273</td>\n",
       "      <td>717</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>274</td>\n",
       "      <td>718</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  file   num\n",
       "0             0   300   648\n",
       "1             1   301   824\n",
       "2             2   302   758\n",
       "3             3   303   985\n",
       "4             4   304   792\n",
       "..          ...   ...   ...\n",
       "270         270   713   790\n",
       "271         271   715  1303\n",
       "272         272   716   969\n",
       "273         273   717   979\n",
       "274         274   718  1281\n",
       "\n",
       "[275 rows x 3 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.read_csv('/hy-tmp/file_count.csv')\n",
    "\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c59a980f-afc7-4271-bc3a-32f76cd97d0d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>189</td>\n",
       "      <td>600</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191</td>\n",
       "      <td>602</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>193</td>\n",
       "      <td>604</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>605</td>\n",
       "      <td>808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>195</td>\n",
       "      <td>606</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>196</td>\n",
       "      <td>607</td>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>198</td>\n",
       "      <td>609</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200</td>\n",
       "      <td>615</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>202</td>\n",
       "      <td>618</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>203</td>\n",
       "      <td>619</td>\n",
       "      <td>1168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>204</td>\n",
       "      <td>620</td>\n",
       "      <td>1158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>205</td>\n",
       "      <td>622</td>\n",
       "      <td>829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>206</td>\n",
       "      <td>623</td>\n",
       "      <td>1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>207</td>\n",
       "      <td>624</td>\n",
       "      <td>849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>208</td>\n",
       "      <td>625</td>\n",
       "      <td>1156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>209</td>\n",
       "      <td>626</td>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>212</td>\n",
       "      <td>629</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>213</td>\n",
       "      <td>631</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>216</td>\n",
       "      <td>634</td>\n",
       "      <td>817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>217</td>\n",
       "      <td>635</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>218</td>\n",
       "      <td>636</td>\n",
       "      <td>1686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>219</td>\n",
       "      <td>637</td>\n",
       "      <td>1245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>220</td>\n",
       "      <td>638</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>221</td>\n",
       "      <td>640</td>\n",
       "      <td>813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>223</td>\n",
       "      <td>649</td>\n",
       "      <td>992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>224</td>\n",
       "      <td>650</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>225</td>\n",
       "      <td>651</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>226</td>\n",
       "      <td>652</td>\n",
       "      <td>953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>229</td>\n",
       "      <td>655</td>\n",
       "      <td>855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>230</td>\n",
       "      <td>656</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>232</td>\n",
       "      <td>658</td>\n",
       "      <td>988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>233</td>\n",
       "      <td>659</td>\n",
       "      <td>1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>235</td>\n",
       "      <td>661</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>237</td>\n",
       "      <td>663</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>238</td>\n",
       "      <td>664</td>\n",
       "      <td>1198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>239</td>\n",
       "      <td>666</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>241</td>\n",
       "      <td>669</td>\n",
       "      <td>1409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>244</td>\n",
       "      <td>676</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>246</td>\n",
       "      <td>679</td>\n",
       "      <td>1238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>248</td>\n",
       "      <td>682</td>\n",
       "      <td>734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>249</td>\n",
       "      <td>683</td>\n",
       "      <td>1283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>252</td>\n",
       "      <td>688</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>253</td>\n",
       "      <td>689</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>254</td>\n",
       "      <td>691</td>\n",
       "      <td>1298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>256</td>\n",
       "      <td>693</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>258</td>\n",
       "      <td>696</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>261</td>\n",
       "      <td>699</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>264</td>\n",
       "      <td>705</td>\n",
       "      <td>786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>266</td>\n",
       "      <td>708</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>267</td>\n",
       "      <td>709</td>\n",
       "      <td>819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>268</td>\n",
       "      <td>710</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>269</td>\n",
       "      <td>712</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>271</td>\n",
       "      <td>715</td>\n",
       "      <td>1303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>272</td>\n",
       "      <td>716</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>273</td>\n",
       "      <td>717</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>274</td>\n",
       "      <td>718</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  file   num\n",
       "0          189   600   685\n",
       "1          191   602   851\n",
       "2          193   604   627\n",
       "3          194   605   808\n",
       "4          195   606   529\n",
       "5          196   607   792\n",
       "6          198   609  1160\n",
       "7          200   615  1226\n",
       "8          202   618   890\n",
       "9          203   619  1168\n",
       "10         204   620  1158\n",
       "11         205   622   829\n",
       "12         206   623  1508\n",
       "13         207   624   849\n",
       "14         208   625  1156\n",
       "15         209   626   611\n",
       "16         212   629  1397\n",
       "17         213   631   619\n",
       "18         216   634   817\n",
       "19         217   635   530\n",
       "20         218   636  1686\n",
       "21         219   637  1245\n",
       "22         220   638   525\n",
       "23         221   640   813\n",
       "24         223   649   992\n",
       "25         224   650  1035\n",
       "26         225   651   586\n",
       "27         226   652   953\n",
       "28         229   655   855\n",
       "29         230   656   763\n",
       "30         232   658   988\n",
       "31         233   659  1278\n",
       "32         235   661  1253\n",
       "33         237   663   894\n",
       "34         238   664  1198\n",
       "35         239   666  1205\n",
       "36         241   669  1409\n",
       "37         244   676   583\n",
       "38         246   679  1238\n",
       "39         248   682   734\n",
       "40         249   683  1283\n",
       "41         252   688  1127\n",
       "42         253   689   890\n",
       "43         254   691  1298\n",
       "44         256   693   691\n",
       "45         258   696   763\n",
       "46         261   699   732\n",
       "47         264   705   786\n",
       "48         266   708   896\n",
       "49         267   709   819\n",
       "50         268   710   833\n",
       "51         269   712  1226\n",
       "52         271   715  1303\n",
       "53         272   716   969\n",
       "54         273   717   979\n",
       "55         274   718  1281"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('/hy-tmp/test_count.csv')\n",
    "                      \n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "08dbed5c-ec8a-4cce-8f78-0b8e5f75f150",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_index = test_df['file'].values\n",
    "\n",
    "np.save('/hy-tmp/test_index',test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "989684d5-8565-4967-b212-2c4b522ea38f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_valid_index= full_df[~full_df['file'].isin(test_index)].file.values\n",
    "\n",
    "np.save('/hy-tmp/train_valid_index',train_valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1c793-20ed-46fc-807f-bfeb60027054",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
